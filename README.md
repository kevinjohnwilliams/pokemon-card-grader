# PokÃ©Grader

AI-powered PokÃ©mon card condition grading from a phone camera photo. Estimates PSA 1â€“10 grades by analyzing centering, corners, edges, and surface quality.

> **Status:** Working prototype â€” card detection and centering analysis are live. Multimodal grading pipeline in development.

## How It Works

Snap a photo of a card with your phone â†’ the app detects the card, crops it, and predicts a PSA-style grade based on the same factors professional graders evaluate.

**What's working now:**
- Card detection and perspective correction from any background
- Real centering analysis with PSA-standard ratios (e.g., 55/45 left/right)
- Mobile-first camera UI with photo quality validation (blur, exposure, glare detection)
- Training data collection pipeline (~1,200+ labeled cards from TAG Grading)

**What's next:**
- Multimodal grading pipeline (vision encoder â†’ agent â†’ composite grade)
- Defect and ding detection from full card images
- Composite grade prediction with per-factor breakdowns and confidence scores

## Architecture: Multimodal Grading Pipeline

PokÃ©Grader uses a multimodal pipeline that mirrors how a human grader evaluates a card â€” combining visual analysis with structured measurements rather than relying on a single end-to-end model.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     QUERY (Input)                       â”‚
â”‚  Raw photo â†’ card detection â†’ crop & perspective fix    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ENCODER (Features)                    â”‚
â”‚  Pretrained vision encoder (ViT / CLIP) produces rich   â”‚
â”‚  feature embeddings from the cropped card image         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AGENT (Reasoning)                     â”‚
â”‚  Orchestrates specialized analyses:                     â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Centering   â”‚  â”‚   Defect /   â”‚  â”‚   Surface    â”‚  â”‚
â”‚  â”‚ (algorithmic)â”‚  â”‚ Ding Detect  â”‚  â”‚  Condition   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                 â”‚                 â”‚           â”‚
â”‚  Fuses visual features + algorithmic results + metadata â”‚
â”‚  (defect counts, centering ratios, wear indicators)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OUTPUT (Grade)                        â”‚
â”‚  PSA 1â€“10 grade + per-factor breakdown + confidence     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Architecture

Early data analysis revealed that TAG Grading's per-feature sub-scores (Fray, Fill, Angle for each corner and edge) have virtually no variance across grade levels â€” a grade 1 card and a grade 10 card both score 994â€“1000 on corners and edges. This made the original plan of training separate CNN sub-models per grading factor unviable.

What *does* carry strong signal: the overall TAG score (0â€“1000), defect counts and types, centering measurements, and the visual appearance of the full card image. The multimodal pipeline exploits all of these by combining a powerful vision encoder with structured data fusion, rather than asking a single model to learn everything from pixels alone.

### Pipeline Stages

**Query (Preprocessing)** â€” The raw phone photo is validated for quality (blur, exposure, glare), then the card is detected, perspective-corrected, and cropped. This gate ensures only usable images reach the model.

**Encoder (Feature Extraction)** â€” A pretrained vision encoder (ViT or CLIP) produces dense feature embeddings from the cropped card. Using a frozen pretrained encoder lets us leverage representations trained on millions of images without needing a massive card-specific dataset.

**Agent (Analysis & Fusion)** â€” The orchestration layer routes encoded features through specialized checks: algorithmic centering analysis (already working), defect/ding detection, and overall condition assessment. It then fuses visual features with structured metadata (defect counts, centering ratios) through a learned fusion head to produce the final grade. This mirrors a human grader's process of evaluating multiple factors and synthesizing a judgment.

**Output** â€” A composite PSA-style grade (1â€“10) with per-factor breakdowns, confidence scores, and interpretable reasoning about what drove the grade.

## Training Strategy

The model trains on two data sources: scraped eBay images of PSA-graded cards and TAG Grading's scoring reports. Validation and testing use our own phone photos submitted to PSA. Augmentation simulates real phone camera conditions (blur, lighting, noise) on the training data to close the gap between clean listing photos and real-world usage.

<p align="center">
  <img src="docs/pipeline-diagram.svg" alt="PokÃ©Grader Training Framework" width="800"/>
</p>

| Split | Source | Purpose |
|-------|--------|---------|
| **Train** | Scraped eBay PSA listings + TAG reports + augmentation | Thousands of images with rich labels |
| **Validate** | Our phone photos â†’ submitted to PSA | Reality check â€” does the model work on real phone cameras? |
| **Test** | Held-out phone photos with PSA grades | Final accuracy measurement, never seen during training |

The feedback loop: as we submit more cards to PSA, the validation and test sets grow, and the model gets retrained with better real-world signal.

## Data Collection

### TAG Grading â€” What's Useful (and What Isn't)

[TAG Grading](https://my.taggrading.com/) provides detailed scoring data for over 591,000 graded PokÃ©mon cards. Through data analysis, we identified which signals are actually useful for training:

**âœ… TAG Score (0â€“1000)** â€” A continuous score mapping to the final 1â€“10 grade. Turns classification into regression, giving gradient signal even between similar cards (e.g., two "10 GEM MINT" cards scoring 985 vs 970).

**âœ… Identified Defects** â€” Specific defect instances with location, category (SURFACE, EDGE, CORNER), type (INK DEFECT, etc.), and close-up images. Defect counts correlate strongly with grade â€” grade 10s have 0â€“1 defects, grade 6s have 4+. This is essentially free defect-detection annotation.

**âœ… Full Card Images** â€” 1,200+ card images across the grade spectrum with known grades, usable as direct training data for the vision encoder.

**âŒ Per-Feature Sub-Scores** â€” Fray, Fill, and Angle scores (each 0â€“1000) for corners and edges show virtually no variance across grades. A grade 1 card scores 994â€“1000 on corners, same as a grade 10. Many cards have these fields completely empty. These sub-scores are **not usable** as independent training labels.

This finding drove the architectural pivot from separate per-factor CNNs to the unified multimodal pipeline described above.

### Data Strategy

**Prioritize grade diversity, not card popularity.** A dinged corner looks the same on a Charizard as it does on a Caterpie. The model needs to learn grading features (edge whitening, corner fraying, surface scratches), not which PokÃ©mon is on the card. Biasing toward popular cards risks overfitting to specific layouts and color patterns.

**Prioritize set diversity.** Different eras have different border styles, holo patterns, print quality, and card stock. Training across multiple sets (Base Set, modern, Japanese, etc.) forces the model to generalize the actual grading signals rather than memorizing set-specific visual patterns.

**Address class imbalance.** The current dataset skews heavily toward grades 8â€“10. Active collection of ~530 additional low-grade cards (especially grades 4 and 7) is in progress to balance the training distribution.

| Priority | What | Why |
|----------|------|-----|
| ğŸ”´ High | Grade distribution (spread across 1â€“10 and 0â€“1000) | Model needs examples of every condition level |
| ğŸ”´ High | Set/era diversity (Base Set, modern, Japanese, etc.) | Generalize across border styles and print quality |
| ğŸ”´ High | Class balance (low-grade card collection) | Prevent model from defaulting to high grades |
| ğŸŸ¡ Medium | Defect type coverage (ink, surface, corner wear, etc.) | Defect detection needs variety |
| ğŸŸ¢ Low | Card popularity (Charizard vs Caterpie) | Grading features are card-agnostic |

## Tech Stack

- **Backend:** Python, FastAPI, OpenCV, PyTorch
- **Frontend:** HTML/JS with browser Camera API (mobile-first)
- **CV Pipeline:** Card detection, perspective correction, border analysis, photo quality validation
- **Vision Encoder:** Pretrained ViT/CLIP (frozen) for feature extraction
- **Fusion Head:** Lightweight MLP combining visual features + structured metadata
- **Centering:** Algorithmic (OpenCV border analysis) â€” no model needed
- **Augmentation:** Phone camera simulation (blur, lighting, rotation, JPEG artifacts, noise)

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run the app
python -m src.api.app
```

Open `http://localhost:8000` on your phone (same network) to start grading.

## Project Structure

```
pokegrader/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ app.py              # FastAPI server
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ grader.py           # Vision encoder + fusion head
â”‚   â”‚   â””â”€â”€ train.py            # Training loop
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ card_detector.py    # Card detection & perspective correction
â”‚       â”œâ”€â”€ centering.py        # Centering analysis (algorithmic)
â”‚       â”œâ”€â”€ grading.py          # Composite grade calculation
â”‚       â”œâ”€â”€ preprocessing.py    # Image preprocessing & quality validation
â”‚       â”œâ”€â”€ pipeline.py         # Data pipeline (capture â†’ PSA â†’ train)
â”‚       â””â”€â”€ augmentation.py     # Phone camera augmentation
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml            # Model & grading config
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Scraped training images
â”‚   â”œâ”€â”€ processed/              # Cropped & organized
â”‚   â””â”€â”€ augmented/              # Augmented training sets
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ static/
â”‚   â””â”€â”€ templates/
â””â”€â”€ tests/
```

## Grading Factors

| Factor | Method | Signal Source | Status |
|--------|--------|--------------|--------|
| Centering | Algorithmic (OpenCV border analysis) | TAG centering measurements | âœ… Working |
| Corners | Vision encoder + fusion head | Full card images + defect metadata | ğŸ”„ Training |
| Edges | Vision encoder + fusion head | Full card images + defect metadata | ğŸ”„ Training |
| Surface | Vision encoder + fusion head | Full card images + defect annotations | ğŸ”„ Training |
| **Composite** | **Agent fusion (visual + structured)** | **All factors combined** | **ğŸ”„ Training** |

> **Note:** Corners, edges, and surface are evaluated holistically by the vision encoder rather than through separate per-factor models, since TAG's per-feature sub-scores lack the variance needed for independent training.

## Disclaimer

Estimated grades for personal reference only. Not affiliated with PSA, BGS, CGC, or any official grading service.